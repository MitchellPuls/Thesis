One of CERNs detectors on the Large Hadron Collider is called ALICE, which stands for A Large Ion Collider Experiment. ALICE is a 10.000 tonne detector, which main purpose is detecting collisions, in particular lead particle collisions. Once these molecules collide they become extremely hot and breakdown to a matter called quarks and gluons. ALICE detects this collision and gathers the data from it. \\
Starting in 2018, ALICE will go offline for a two year break to upgrade it's software. A piece of this software is a load balancing algorithm. When ALICE is running, it will pump out 1.1TB/S of data. This algorithm is supposed to be able to efficiently distribute this data over a total of 1750 computers. 250 of these computers are whats called First Level Processors, and the other 1500 are called Event Processing Nodes. All and all these computers are meant to process, compress and store the data so that it can be recreated later on. \\
This data is distributed with heartbeats. These heartbeats occur every 20ms. FLPs will gather the data between two heartbeats, and then compresses them into a Sub Time Frame. These STFs are then send to the EPNs. Here they will be compressed further down into a Time Frame. When an EPN is disabled, it should not receive any data anymore, at which it should be removed from the data line. This whole process of distributing data both efficiently and correctly is called Load Balancing. \\
A previous experiment done by Heiko van der Heijden showed that a Blacklist algorithm is a more efficient algorithm for this load balancing application, compared to a Re-initialization algorithm. The experiment was conducted using a cluster available on Nikhef. The amount of units used during the experiment was 2 FLPs and 12 EPNs, which is equivalent to the same ratio that is used at CERN. The availability of this cluster was uncertain, and hence a cluster was needed that was more available, and also expandable. This resulted in a cluster of Raspberry Pi's. \\
In order to validate this cluster compared to the one on Nikhef, a correlation has to be found between the results from the cluster and the new one. Afterwards these experiments can be executed with a unit count of 3 FLPs and 18 EPNs, and 4 FLPs and 24 EPNs. \\
The software used consists of two main frameworks, FairMQ and Zookeeper. FairMQ is a data transport layer of the larger framework FairRoot. For this experiment a specially made trimmed down version was used in order to accommodate with the Raspberry Pi. Zookeeper is the load balancing part of the software. Here the registration of online and offline EPNs are done so that the FLPs know what to send where. Zookeeper runs on a special computer called the Information Node. \\
At first this new prototype using Raspberry Pi's had to be developed and build. The complications laid in the Raspberry Pi having only one Ethernet connection natively, and the software needing changes to work with an ARM architecture. After that, three experiments were conducted that were the same as previous experiment. The first one disables one EPN during the run, the second disables every EPN apart from one during a run, and the last one disables every EPN during a run, but also uses a random sample size. These three experiments are then analyzed with the previous experiment to calculate a correlation between them. After that the same three experiments are conducted again with a higher amount of FLPs and EPNs. Finally two new experiments are run, where multiple EPNs will disable at once. During the first one these EPNs will all be after one another in the list of available EPNs. During the second one these EPNs will be scattered in the list of available EPNs. This to check whether the Blacklist algorithm has a different results depending on the system architecture.