Eén van de detectoren op de Large Hadron Collider bij CERN is genaamd ALICE, wat staat voor A Large Ion Collider Experiment. Her primaire doel van ALICE is voor de detectie van botsingen, en dan vooral de botsingen tussen lood deeltjes. Wanneer deze deeltjes botsen worden ze extreem heet, en breken ze af in een materie die quark-gluon plasma heet. ALICE detecteerd deze botsingen en registreerd de data ervan.\\
Aan het eind van 2018 gaa ALICE offline voor een twee jaar durende upgrade aan de software. Een deel van deze upgrade is een Load Balancing algoritme. Tijdens een run van ALICE komt er een data stream van 1.1TB/S. Het algoritme is bedoeld voor de efficiënte distributie van deze datas naar 1750 computers. Deze computers zijn onderverdeeld in 250 First Level Processors, en 1500 Event Processing Nodes. Deze computers zorgen ervoor dat de data gecomprimeerd wordt en wordt opgeslagen zodat het later gereconstrueerd kan worden.\\
Deze data komt in heartbeats. Een heartbeat is elke 20ms. De FLPs pakken de data tussen twee heartbeats en comprimeerd deze in een Sub Time Frame. Deze STFs worden dan doorgestuurd naar de EPNs. Hier worden ze weer verder gecomprimeerd in een Time Frame, en worden ze opgeslagen. Wanneer een EPN uitvalt moet die geen data meer ontvangen en zal hij dus van de datalijn af moeten. Wanneer dit gebeurt heet dat een fail-over. De distributie van deze data heet Load Balancing.\\
In een vorig experiment dat uitgevoerd was door Heiko van der Heijden kwam eruit dat een Blacklist algoritme een efficiënter algoritme is voor deze load balancing applicatie. Dit experiment was uitgevoerd met een cluster aan computers die in bruikleen waren van Nikhef. Tijdens dit experiment werd er gebruik gemaakt van 16 computers, waarvan 12 de rol van EPN hadden, en 2 de rol van FLP. Deze ratio van FLP:EPN is gelijk aan de ratio die gebruikt wordt bij CERN. De bruikbaarheid van deze cluster was niet gegarandeerd, en er was dus de vraag naar een cluster die bruikbaarder was, maar die ook makkelijker uitgebreid kon worden. Dit resulteerde in een cluster van Raspberry Pi's.\\
Om deze cluster te valideren tegen de cluster die gebruikt was op Nikhef, moest er een correlatie gevonden worden tussen de resultaten van de twee clusters. Als deze correlatie er is, dan betekent dat, dat het het aantal van de FLPs en EPNs uitgebreid kan worden om te kijken of het algoritme bij een hoger aantal nog steeds hetzelfde werkt.\\
De software die gebruikt wordt bestaat grotendeels uit twee frameworks, FairMQ en Zookeeper. FairMQ is de data transport laag van een groter framework genaamd FairRoot. Voor dit experiment is een speciaal gemaakte aftakking gebruikt van FairMQ omdat de rekenkracht van de Raspberry Pi lager ligt dan die van de cluster van Nikhef. Zookeeper regelt de Blacklist in het algoritme. Zookeeper regelt de registratie van EPNs die online of offline zijn en stuurt dit door naar de FLPs zodat die weten naar welke EPNs ze data kunnen versturen. Zookeeper loopt op een aparte computer genaamd de Information Node.\\
In eerste instantie moest de nieuwe cluster gemaakt worden. De complicaties waren vooral het feit dat de Raspberry Pi maar één Ethernet poort heeft standaard, en dat alle software omgebouwd moest worden om te werken met een ARM architectuur. Hierna moesten dezelfde drie experimenten uitgevoerd worden die tijdens het vorige experiment ook uitgevoerd waren. Tijdens het eerste experiment wordt er één EPN uitgeschakeld. Het tweede experiment schakelt elke EPN uit op één na, en het derde experiment schakelt elke EPN uit op één na maar gebruikt ook een random waarde voor de hoeveelheid data. Deze resulten zijn toen vergeleken met de resulltaten van het vorige onderzoek om te kijken naar een correlatie tussen de data. Hierna werden de drie experimenten opnieuw uitgevoerd met hoger aantal FLPs en EPNs (3:18 en 4:24) om te kijken of het uitbreiden van het experiment een effect heeft op het algoritme en de Information Node. Op het laatst is er ook een extra experiment uitgevoerd waarbij meerdere EPNs tegelijkertijd uitviel. Het doel van deze experimenten was om te kijken of de lijst van EPNs een invloed had op het data verlies. Het eerste experiment liet vier EPNs uitvallen die achter elkaar in de lijst stonden. Het tweede experiment liet vier EPNs uitvallen op willekeurige plekken in de lijst.\\
De conclusie is, is dat het nieuwe prototype een valide alternatief is. De drie experimenten hadden onderling een Pearson correlatie van 0,95; 0,99 en 0,99. Deze correlatie waardes spreken allemaal de nul hypothese tegen dat er geen correlatie is tussen de clusters. Daarnaast liet de uitbreiding van het experiment zien dat het aantal data verlies aleen maar dichterbij 1 kwam, wat gelijk staat aan het minimale data verlies met hoe de setup is op het moment. Het laatste experiment liet zien dat het data verlies voor individuele clusters die uitgeschakeld werden verschillend was, maar dat het totale data verlies weer gelijk aan elkaar was.\\
Het is aanbevolen om het experiment opnieuw uit te voeren met nog meer FLPs en EPNs om te kijken of het data verlies uiteindelijk 1 wordt, om te kijken of er misschien een asymptoot is ergens, of om te kijken of het misschien weer omhoog gaat en er een sinus vorming ontstaat. Daarnaast is het aanbevolen om deze experimenten voortaan uit te voeren met fail-overs die geïnitieerd worden door Ansible in plaats van Time Frames. Hiermee elimineer je de voorspelbaarheid van de fail-overs en elimineer je het verlies van de Time Frame die je altijd kwijt bent met de huidige setup. Tenslotte is het aanbevolen om te kijken naar een lowest latency aanpak voor de data distributie tegenover een round robin die nu gebruikt wordt. 